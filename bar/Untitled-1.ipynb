{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TStarFramework(nn.Module):\n",
    "    \"\"\"\n",
    "    A unified framework for video-based question answering, combining grounding, \n",
    "    keyframe searching, and reasoning in a modular pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrain_path_or_name=\"mll-lab/tstar-v1\", config: transformers.PretrainedConfig = None):\n",
    "        \"\"\"\n",
    "        Initializes the TStarFramework with pre-trained modules for grounding, searching, and reasoning.\n",
    "        Args:\n",
    "            pretrain_path_or_name (str): Path or model name for pretrained TStar components.\n",
    "            config (transformers.PretrainedConfig): Optional configuration for the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.query_grounder = TStarGrounder(model_path=pretrain_path_or_name, model_base=config)\n",
    "        self.searcher = None  # VideoSearcher will be initialized dynamically with video-specific parameters\n",
    "        # self.reasoner = TStarGrounder(model_path=pretrain_path_or_name, model_base=config)\n",
    "        self.reasoner = self.query_grounder\n",
    "        self.initialize_searcher(video_path)\n",
    "    def initialize_searcher(self, video_path: str, search_nframes=8, target_objects=[], cue_objects=[], image_grid_shape=(8,8), threshold=0.6):\n",
    "        \"\"\"\n",
    "        Dynamically initializes the VideoSearcher for the given video path.\n",
    "        Args:\n",
    "            video_path (str): Path to the video file.\n",
    "        \"\"\"\n",
    "        self.searcher = TStarSearcher(\n",
    "            video_path=video_path,\n",
    "            target_objects=target_objects,\n",
    "            cue_objects=target_objects,\n",
    "            search_nframes=search_nframes,\n",
    "            image_grid_shape=image_grid_shape,\n",
    "            confidence_threshold=threshold\n",
    "        )\n",
    "    def forward(self, frames: List[Image.Image], question: str, options: str, video_path: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Runs the full pipeline for question answering over video data.\n",
    "        Args:\n",
    "            frames (List[Image.Image]): Initial frames for grounding.\n",
    "            question (str): The question to be answered.\n",
    "            options (str): Multiple-choice options for the question.\n",
    "            video_path (str): Path to the video file.\n",
    "        Returns:\n",
    "            Dict[str, any]: A dictionary containing the answer, keyframes, timestamps, and grounding results.\n",
    "        \"\"\"\n",
    "        # Step 1: Grounding (Query Understanding)\n",
    "        grounding_results = self.query_grounder.inference_query_grounding(frames, question)\n",
    "        target_objects = grounding_results[\"target_objects\"]\n",
    "        cue_objects = grounding_results[\"cue_objects\"]\n",
    "        print(f\"Grounding Results - Targets: {target_objects}, Cues: {cue_objects}\")\n",
    "        # Step 2: Reset Detect Objects in YOLO\n",
    "        grounding_results = self.query_grounder.inference_query_grounding(frames, question)\n",
    "        target_objects = grounding_results[\"target_objects\"]\n",
    "        cue_objects = grounding_results[\"cue_objects\"]\n",
    "        print(f\"Grounding Results - Targets: {target_objects}, Cues: {cue_objects}\")\n",
    "        # Step 2: Keyframe Searching\n",
    "        self.searcher.target_objects = target_objects\n",
    "        self.searcher.cue_objects = cue_objects\n",
    "        keyframes, timestamps = self.searcher.search()\n",
    "        print(f\"Keyframes Found: {len(keyframes)}, Timestamps: {timestamps}\")\n",
    "        # Step 3: Reasoning (QA Inference)\n",
    "        answer = self.reasoner.inference_qa(keyframes, question, options)\n",
    "        print(f\"Reasoning Results - Answer: {answer}\")\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"keyframes\": keyframes,\n",
    "            \"timestamps\": timestamps,\n",
    "            \"grounding_results\": grounding_results\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
